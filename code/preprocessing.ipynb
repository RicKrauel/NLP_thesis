{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from pandarallel import pandarallel\n",
    "import string\n",
    "from HanTa import HanoverTagger as ht\n",
    "import mgzip\n",
    "import pickle\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing of data sets\n",
    "This notebook contains the necessary steps for the preprocessing of the data sets. It processes the data sets for both types of data, comments and articles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing of articles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load csv of file to process\n",
    "zeit = pd.read_csv('../data/zeit_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])\n",
    "welt = pd.read_csv('../data/welt_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])\n",
    "tagesspiegel = pd.read_csv('../data/tagesspiegel_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "zeit['newspaper'] = 'zeit'\n",
    "welt['newspaper'] = 'welt'\n",
    "tagesspiegel['newspaper'] = 'tagespiegel'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "combined_news = pd.concat([zeit, welt, tagesspiegel])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# there was one document that contained a lot of arabic characters, thus it was removed\n",
    "combined_news = combined_news[combined_news['combined_text'].str.contains('في')==False ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize, remove punctuation & lower casing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/28432 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91b096c6e7634dfa924eb41ab590d41c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_news['text_token'] = combined_news['combined_text'].progress_apply(nltk.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "punctuation_custom = list(string.punctuation)\n",
    "punctuation_custom = punctuation_custom + ['„', '“', '–', '•']\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_list = []\n",
    "    for word in text:\n",
    "        for punctuation in punctuation_custom:\n",
    "            word = word.replace(punctuation, '')\n",
    "        if not word:\n",
    "            continue\n",
    "        text_list.append(word.lower())\n",
    "    return text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/28432 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ec5a92dd81d425a908e072bd92da553"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].progress_apply(remove_punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stop word removal, lemmatization of comments & emotion lexicon\n",
    "Following, stopwords without meaning ('der', 'wo', etc.) are removed from the comments. I edited the sourced stopwords list and removed any negations, since these contain emotions that I want to capture later in the workflow. Afterwards, the remaining tokenized words for each comment are lemmatized, i.e. reduced to their basic form. In order to do this I utilise the 'Hannover Tagger', which also provides part of speech information, although I do not use it (the main reason that I do not use the PoS information is that the utilised German emotion and sentiment lexical use different PoS abbreviations, resulting in no matches)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "stopwords = open('../resources/german_stopwords-master/german_stopwords_topic.txt').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def stop_word_removal(x):\n",
    "    return list([str(w) for w in x if not w in stopwords])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/28432 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ba255c85efa4ac68c7880941e51749d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].progress_apply(stop_word_removal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def tagger_custom(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        tmp_list.append(tagger.analyze(word)[0].lower())\n",
    "    return tmp_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3554), Label(value='0 / 3554'))), …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b594876105d457ebb79f9c1c39fa7fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].parallel_apply(tagger_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "combined_news['combined_text_joined'] = combined_news['text_token'].apply(' '.join)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "combined_news_pre = combined_news[['title', 'combined_text_joined', 'text_token', 'date', 'newspaper']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "combined_news_pre = combined_news_pre.drop_duplicates(subset=['date', 'title', 'combined_text_joined'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# saving preprocessed file\n",
    "with mgzip.open('../data/combined_news_pre.mgzip', 'wb') as handle:\n",
    "    pickle.dump(combined_news, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing of comments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tagesspiegel = pd.read_csv('../data/tagesspiegel_scraped.gzip', compression='gzip', low_memory=False)\n",
    "zeit = pd.read_csv('../data/zeit_scraped.gzip', compression='gzip', low_memory=False)\n",
    "welt = pd.read_csv('../data/welt_scraped.gzip', compression='gzip', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# columns initially scraped but not necessary for the analysis anymore are deleted\n",
    "# the article text is removed since this notebook is only for the sentiment analysis\n",
    "# the article title serves as the ID of each article, since it is shorter than the article text\n",
    "tagesspiegel.drop(['Unnamed: 0', 'combined_text', 'link'], axis=1, inplace=True)\n",
    "zeit.drop(['Tag0', 'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'combined_text', 'link'], axis=1, inplace=True)\n",
    "welt.drop(['Tag0', 'Tag1', 'Tag2', 'Tag3', 'combined_text', 'link'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "comment_cols_welt = [col for col in welt.columns if 'Comment' in col]\n",
    "comment_cols_zeit = [col for col in zeit.columns if 'Comment' in col]\n",
    "comment_cols_tagesspiegel = [col for col in tagesspiegel.columns if 'Comment' in col]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "welt = pd.melt(welt, id_vars=['title', 'date'], value_vars=comment_cols_welt)\n",
    "zeit = pd.melt(zeit, id_vars=['title', 'date'], value_vars=comment_cols_zeit)\n",
    "tagesspiegel = pd.melt(tagesspiegel, id_vars=['title', 'date'], value_vars=comment_cols_tagesspiegel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "welt.dropna(inplace=True)\n",
    "zeit.dropna(inplace=True)\n",
    "tagesspiegel.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "welt['newspaper'] = 'welt'\n",
    "zeit['newspaper'] = 'zeit'\n",
    "tagesspiegel['newspaper'] = 'tagespiegel'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "combined_comments = pd.concat([zeit, welt, tagesspiegel])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization & punctuation removal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1776963 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340f3a939e8841948d501dbec409fa04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_comments['tokens'] = combined_comments['value'].progress_apply(nltk.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "punctuation_custom = list(string.punctuation)\n",
    "punctuation_custom = punctuation_custom + ['„', '“', '–', '•']\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_list = []\n",
    "    for word in text:\n",
    "        for punctuation in punctuation_custom:\n",
    "            word = word.replace(punctuation, '')\n",
    "        if not word:\n",
    "            continue\n",
    "        text_list.append(word.lower())\n",
    "    return text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1776963 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91623373183e4f02beb1f729ee12eb50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_comments['tokens'] = combined_comments['tokens'].progress_apply(remove_punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stop word removal & lemmatization\n",
    "Following, stopwords without meaning ('der', 'wo', etc.) are removed from the comments. I edited the sourced stopwords list and removed any negations, since these contain emotions that I want to capture later in the workflow. Afterwards, the remaining tokenized words for each comment are lemmatized, i.e. reduced to their basic form. In order to do this I utilise the 'Hannover Tagger', which also provides part of speech information, although I do not use it (the main reason that I do not use the PoS information is that the utilised German emotion and sentiment lexical use different PoS abbreviations, resulting in no matches).'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "stopwords = open('../resources/german_stopwords-master/german_stopwords_sentiment.txt').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def stop_word_removal(x):\n",
    "    return list([w for w in x if not w in stopwords])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1776963 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfd114f86eeb4ab58437fd5c0cf6bca7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_comments['tokens'] = combined_comments['tokens'].progress_apply(stop_word_removal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def tagger_custom(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        tmp_list.append(tagger.analyze(word)[0].lower())\n",
    "    return tmp_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1776963 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f1173a6bb8a4e158923e21bf3b20e74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_comments['tokens'] = combined_comments['tokens'].progress_apply(tagger_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_comments_pre = combined_comments.drop_duplicates(subset=['date', 'variable', 'value'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# saving preprocessed file\n",
    "with mgzip.open('../data/combined_comments_pre.mgzip', 'wb') as handle:\n",
    "    pickle.dump(combined_comments_pre, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}