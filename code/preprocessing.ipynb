{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from pandarallel import pandarallel\n",
    "import string\n",
    "from HanTa import HanoverTagger as ht\n",
    "import mgzip\n",
    "import pickle\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing of data sets\n",
    "This notebook contains the necessary steps for the preprocessing of the data sets. It processes the data sets for both types of data, comments and articles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing of articles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load csv of file to process\n",
    "zeit = pd.read_csv('../data/zeit_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])\n",
    "welt = pd.read_csv('../data/welt_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])\n",
    "tagesspiegel = pd.read_csv('../data/tagesspiegel_scraped.gzip', compression='gzip', low_memory=False, usecols=[\"title\", \"date\", \"combined_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zeit['newspaper'] = 'zeit'\n",
    "welt['newspaper'] = 'welt'\n",
    "tagesspiegel['newspaper'] = 'tagespiegel'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news = pd.concat([zeit, welt, tagesspiegel])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# there was one document that contained a lot of arabic characters, thus it was removed\n",
    "combined_news = combined_news[combined_news['combined_text'].str.contains('في')==False ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize, remove punctuation & lower casing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news['text_token'] = combined_news['combined_text'].progress_apply(nltk.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "punctuation_custom = list(string.punctuation)\n",
    "punctuation_custom = punctuation_custom + ['„', '“', '–', '•']\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_list = []\n",
    "    for word in text:\n",
    "        for punctuation in punctuation_custom:\n",
    "            word = word.replace(punctuation, '')\n",
    "        if not word:\n",
    "            continue\n",
    "        text_list.append(word.lower())\n",
    "    return text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].progress_apply(remove_punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stop word removal, lemmatization of comments & emotion lexicon\n",
    "Following, stopwords without meaning ('der', 'wo', etc.) are removed from the comments. I edited the sourced stopwords list and removed any negations, since these contain emotions that I want to capture later in the workflow. Afterwards, the remaining tokenized words for each comment are lemmatized, i.e. reduced to their basic form. In order to do this I utilise the 'Hannover Tagger', which also provides part of speech information, although I do not use it (the main reason that I do not use the PoS information is that the utilised German emotion and sentiment lexical use different PoS abbreviations, resulting in no matches)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = open('../resources/german_stopwords-master/german_stopwords_topic.txt').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stop_word_removal(x):\n",
    "    return list([str(w) for w in x if not w in stopwords])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].progress_apply(stop_word_removal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tagger_custom(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        tmp_list.append(tagger.analyze(word)[0].lower())\n",
    "    return tmp_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news['text_token'] = combined_news['text_token'].parallel_apply(tagger_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news['combined_text_joined'] = combined_news['text_token'].apply(' '.join)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_news_pre = combined_news[['title', 'combined_text_joined', 'date', 'newspaper']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving preprocessed file\n",
    "with mgzip.open('../data/combined_news_pre.mgzip', 'wb') as handle:\n",
    "    pickle.dump(combined_news, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing of comments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}