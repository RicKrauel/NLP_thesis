{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "from HanTa import HanoverTagger as ht\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from typing import List\n",
    "import re\n",
    "import pickle\n",
    "import mgzip\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment classification of comments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lexicon approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load command\n",
    "with mgzip.open('../data/combined_comments_pre.mgzip', 'rb') as handle:\n",
    "    combined_comments_pre = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "combined_comments_pre = combined_comments_pre.reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "senti_merge = pd.read_csv('../resources/sentimerge/data/sentimerge.txt', sep='\\t')\n",
    "senti_merge.drop(['PoS'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "senti_merge_pos = senti_merge.loc[senti_merge['sentiment'] > 0]\n",
    "senti_merge_neg = senti_merge.loc[senti_merge['sentiment'] < 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "senti_pos_dict = dict(zip(senti_merge_pos['lemma'], senti_merge_pos['sentiment']))\n",
    "senti_neg_dict = dict(zip(senti_merge_neg['lemma'], senti_merge_neg['sentiment']))\n",
    "senti_weight_dict = dict(zip(senti_merge['lemma'], senti_merge['weight']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "negation checker inspired by HD students not worth it (https://github.com/text-analytics-20/news-sentiment-development/blob/main/sentiment_analysis/negation_handling.py)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def senti_class(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        if not word:\n",
    "            continue\n",
    "        if str(word).lower() in senti_pos_dict:\n",
    "            tmp_list.append((senti_pos_dict[word.lower()]*senti_weight_dict[word.lower()]))\n",
    "        if str(word).lower() in senti_neg_dict:\n",
    "            tmp_list.append((senti_neg_dict[word.lower()]*senti_weight_dict[word.lower()]))\n",
    "    if not tmp_list:\n",
    "        return np.nan\n",
    "    return sum(tmp_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def rescale_neg(input):\n",
    "    return (2 *(input - min(input))/(max(input)-min(input))) - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def rescale(input):\n",
    "    scale = input\n",
    "    scale -= scale.min()\n",
    "    scale /= scale.max()\n",
    "    return scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def lexi_label(input):\n",
    "    if input > 1:\n",
    "        return 'positive'\n",
    "    elif input < -1:\n",
    "        return 'negative'\n",
    "    elif pd.isna(input):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return 'neutral'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1772864 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "523b6417e6984e6187b76e84daca2c2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_comments_pre['lexi_score'] = combined_comments_pre['tokens'].progress_apply(senti_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.margins(0)\n",
    "plt.hist(combined_comments_pre[\"lexi_score\"].loc[(combined_comments_pre['lexi_score'] > -10) | (combined_comments_pre['lexi_score'] < 10)], bins=100, range=(-10,10))\n",
    "plt.savefig('../figures/lexi_neutral.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "combined_comments_pre['lexi_label'] = combined_comments_pre['lexi_score'].apply(lexi_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "combined_comments_pre['lexi_score'] = rescale(combined_comments_pre['lexi_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "combined_comments_lexi = combined_comments_pre[['title', 'lexi_score', 'lexi_label', 'newspaper']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT approach\n",
    "This step was calculated on Google Colab cloud due to taking way too long on my laptop."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class SentimentModel():\n",
    "    def __init__(self, model_name: str):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
    "        self.clean_http_urls = re.compile(r'https*\\\\S+', re.MULTILINE)\n",
    "        self.clean_at_mentions = re.compile(r'@\\\\S+', re.MULTILINE)\n",
    "\n",
    "    @staticmethod\n",
    "    def probs2polarities(pnn: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Transform softmax probs of a [positive, negative, neutral] classifier\n",
    "        into scalar polarity scores of range [-1, 1].\n",
    "        High values express positive sentiment, low negative ones negative sentiment.\n",
    "        Values close to 0 express neutral sentiment.\"\"\"\n",
    "        pos = pnn[:, 0]\n",
    "        neg = pnn[:, 1]\n",
    "        # Transform range [0, 1] to [-1, 1]\n",
    "        # Ignore neutrality score as it's implicitly encoded as (1 - pos - neg)\n",
    "        polarities = pos - neg\n",
    "        return polarities\n",
    "\n",
    "    def predict_sentiment(self, texts):\n",
    "        if np.any(pd.isna(texts)):\n",
    "          return np.nan\n",
    "        texts = [self.clean_text(str(text)) for text in [texts]]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        encoded = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True,truncation=True, return_tensors=\"pt\")\n",
    "        encoded = encoded.to(self.device)\n",
    "        with torch.no_grad():\n",
    "                logits = self.model(**encoded)\n",
    "                probs = torch.nn.functional.softmax(logits[0], dim=1)\n",
    "\n",
    "        polarities = self.probs2polarities(probs)\n",
    "\n",
    "        label_ids = torch.argmax(logits[0], axis=1)\n",
    "        for a in zip(logits[0].tolist(), [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]):\n",
    "            tmp_rsl = list(a)\n",
    "        return tmp_rsl, polarities.item()\n",
    "\n",
    "    def replace_numbers(self,text: str) -> str:\n",
    "            return text.replace(\"0\",\" null\").replace(\"1\",\" eins\").replace(\"2\",\" zwei\").replace(\"3\",\" drei\").replace(\"4\",\" vier\").replace(\"5\",\" fünf\").replace(\"6\",\" sechs\").replace(\"7\",\" sieben\").replace(\"8\",\" acht\").replace(\"9\",\" neun\")\n",
    "\n",
    "    def clean_text(self,text: str)-> str:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text = self.clean_http_urls.sub('',text)\n",
    "            text = self.clean_at_mentions.sub('',text)\n",
    "            text = self.replace_numbers(text)\n",
    "            text = self.clean_chars.sub('', text) # use only text chars\n",
    "            text = ' '.join(text.split()) # substitute multiple whitespace with single whitespace\n",
    "            text = text.strip().lower()\n",
    "            return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# oliverguhr/german-sentiment-bert\n",
    "model = SentimentModel(model_name = \"mdraw/german-news-sentiment-bert\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=221608), Label(value='0 / 221608')…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b77fc238b447aa8a92cafd71184d2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/series.py\", line 4357, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_15837/353707087.py\", line 36, in predict_sentiment\n",
      "    logits = self.model(**encoded)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [107]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m combined_comments_pre[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mcombined_comments_pre\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallel_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_sentiment\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py:417\u001B[0m, in \u001B[0;36mparallelize_with_pipe.<locals>.closure\u001B[0;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001B[0m\n\u001B[1;32m    412\u001B[0m generation \u001B[38;5;241m=\u001B[39m count()\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[1;32m    415\u001B[0m     (worker_status \u001B[38;5;241m==\u001B[39m WorkerStatus\u001B[38;5;241m.\u001B[39mRunning \u001B[38;5;28;01mfor\u001B[39;00m worker_status \u001B[38;5;129;01min\u001B[39;00m workers_status)\n\u001B[1;32m    416\u001B[0m ):\n\u001B[0;32m--> 417\u001B[0m     message: Tuple[\u001B[38;5;28mint\u001B[39m, WorkerStatus, Any] \u001B[38;5;241m=\u001B[39m \u001B[43mmaster_workers_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m     worker_index, worker_status, payload \u001B[38;5;241m=\u001B[39m message\n\u001B[1;32m    419\u001B[0m     workers_status[worker_index] \u001B[38;5;241m=\u001B[39m worker_status\n",
      "File \u001B[0;32m<string>:2\u001B[0m, in \u001B[0;36mget\u001B[0;34m(self, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/managers.py:835\u001B[0m, in \u001B[0;36mBaseProxy._callmethod\u001B[0;34m(self, methodname, args, kwds)\u001B[0m\n\u001B[1;32m    832\u001B[0m     conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tls\u001B[38;5;241m.\u001B[39mconnection\n\u001B[1;32m    834\u001B[0m conn\u001B[38;5;241m.\u001B[39msend((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id, methodname, args, kwds))\n\u001B[0;32m--> 835\u001B[0m kind, result \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#RETURN\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    838\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/connection.py:250\u001B[0m, in \u001B[0;36m_ConnectionBase.recv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 250\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recv_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _ForkingPickler\u001B[38;5;241m.\u001B[39mloads(buf\u001B[38;5;241m.\u001B[39mgetbuffer())\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/connection.py:414\u001B[0m, in \u001B[0;36mConnection._recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_recv_bytes\u001B[39m(\u001B[38;5;28mself\u001B[39m, maxsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 414\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    415\u001B[0m     size, \u001B[38;5;241m=\u001B[39m struct\u001B[38;5;241m.\u001B[39munpack(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!i\u001B[39m\u001B[38;5;124m\"\u001B[39m, buf\u001B[38;5;241m.\u001B[39mgetvalue())\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/connection.py:379\u001B[0m, in \u001B[0;36mConnection._recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    377\u001B[0m remaining \u001B[38;5;241m=\u001B[39m size\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m remaining \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 379\u001B[0m     chunk \u001B[38;5;241m=\u001B[39m \u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(chunk)\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/data_types/series.py\", line 26, in work\n",
      "    return data.apply(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1043, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 1545, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/apply.py\", line 1098, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_15837/353707087.py\", line 36, in predict_sentiment\n",
      "    logits = self.model(**encoded)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 989, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/core.py\", line 158, in __call__\n",
      "    results = self.work_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandarallel/progress_bars.py\", line 210, in closure\n",
      "    return user_defined_function(\n",
      "  File \"/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_15837/353707087.py\", line 36, in predict_sentiment\n",
      "    logits = self.model(**encoded)\n"
     ]
    }
   ],
   "source": [
    "combined_comments_pre['bert'] = combined_comments_pre['value'].progress_apply(model.predict_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def bert_score(input):\n",
    "    return np.ptp(sorted(list(map(abs, input[0][0]))))\n",
    "\n",
    "def bert_label(input):\n",
    "    return input[0][1]\n",
    "\n",
    "def bert_props(input):\n",
    "    return input[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "combined_comments_pre['bert_conf'] = combined_comments_pre['bert'].apply(bert_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "combined_comments_pre['bert_label'] = combined_comments_pre['bert'].apply(bert_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "combined_comments_pre['bert_score'] = combined_comments_pre['bert'].apply(bert_props)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_comments_bert = combined_comments_pre[['title', 'bert_score', 'bert_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mgzip.open(\".../data/comments_bert.mgzip\", 'wb') as f:\n",
    "    pickle.dump(combined_comments_bert, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next steps are combining the individual files created on Google Colab into one combined file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "with mgzip.open('../data/tagesspiegel_bert.mgzip', 'rb') as handle:\n",
    "    tagesspiegel_bert = pickle.load(handle)\n",
    "with mgzip.open('../data/zeit_bert.mgzip', 'rb') as handle:\n",
    "    zeit_bert = pickle.load(handle)\n",
    "with mgzip.open('../data/welt_bert.mgzip', 'rb') as handle:\n",
    "    welt_bert = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "tagesspiegel_bert['newspaper'] = 'tagespiegel'\n",
    "zeit_bert['newspaper'] = 'zeit'\n",
    "welt_bert['newspaper'] = 'welt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "combined_comments_bert = pd.concat([zeit_bert, welt_bert, tagesspiegel_bert])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine lexi & bert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "combined_comments_pre['index'] = combined_comments_pre.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "combined_comments_bert['index'] = combined_comments_bert.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "combined_comments_senti = pd.merge(combined_comments_pre, combined_comments_bert[['title', 'index', 'bert_score','bert_label']], on=['title', 'index'], how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "combined_comments_senti = combined_comments_senti[['title', 'newspaper', 'variable', 'value', 'lexi_score', 'lexi_label', 'bert_score', 'bert_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# saving preprocessed file\n",
    "with mgzip.open('../data/combined_comments_senti.mgzip', 'wb') as handle:\n",
    "    pickle.dump(combined_comments_senti, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSentiA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "combined_comments_senti['vote'] = np.where((combined_comments_senti['lexi_score'] > combined_comments_senti['bert_score']), 'lexi', 'bert')\n",
    "combined_comments_senti['vote_score'] = np.where((combined_comments_senti['lexi_score'] > combined_comments_senti['bert_score']), combined_comments_senti['lexi_score'], combined_comments_senti['bert_score'])\n",
    "combined_comments_senti['vote_label'] = np.where((combined_comments_senti['lexi_score'] > combined_comments_senti['bert_score']), combined_comments_senti['lexi_label'], combined_comments_senti['bert_label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     title    newspaper  \\\n0        Handel: \"Viele brauchen ja eigentlich nichts m...         zeit   \n1        DGB-Index Gute Arbeit: An den Kosten im Homeof...         zeit   \n2        Coronavirus: RKI registriert erstmals wieder l...         zeit   \n3        Bund-Länder-Runde: Kanzleramtschef fordert Cor...         zeit   \n4        Corona-Impfung für Kinder: Gesundheitsminister...         zeit   \n...                                                    ...          ...   \n1772859            Inzidenz in Berlin steigt wieder leicht  tagespiegel   \n1772860            Inzidenz in Berlin steigt wieder leicht  tagespiegel   \n1772861            Inzidenz in Berlin steigt wieder leicht  tagespiegel   \n1772862            Inzidenz in Berlin steigt wieder leicht  tagespiegel   \n1772863            Inzidenz in Berlin steigt wieder leicht  tagespiegel   \n\n            variable                                              value  \\\n0           Comment0  \\nSehr schönes Interview, Danke!\\nEs ist eben ...   \n1           Comment0  \\nHome Office wo und wann immer möglich UND fa...   \n2           Comment0  \\nWie ich es voraus gesagt habe.\\nEs zeichnete...   \n3           Comment0  \\nEntfernt. Bitte verzichten Sie auf Unterstel...   \n4           Comment0  \\nWarum schafft man eigentlich nicht endlich d...   \n...              ...                                                ...   \n1772859  Comment6184  Nach allem, was man hinsichtlich der Entwicklu...   \n1772860  Comment6185  Man kann nur hoffen,daß dem Berliner Senat mög...   \n1772861  Comment6186  Massentest bringen natürlich etwas: Die Verlau...   \n1772862  Comment6187  Wenn schon Shutdown, dann richtig. Dann ist di...   \n1772863  Comment6188  Unser kleiner Blumenladen, in CH, muss demnach...   \n\n         lexi_score lexi_label  bert_score bert_label  vote  vote_score  \\\n0          0.606474   positive    0.197398    neutral  lexi    0.606474   \n1          0.716222   positive    0.395624   positive  lexi    0.716222   \n2          0.581695   positive    0.561593   positive  lexi    0.581695   \n3          0.568505   positive    0.323396   negative  lexi    0.568505   \n4          0.539759   negative    0.397975   negative  lexi    0.539759   \n...             ...        ...         ...        ...   ...         ...   \n1772859    0.530947   negative    0.249955   negative  lexi    0.530947   \n1772860    0.568764   positive    0.236482   negative  lexi    0.568764   \n1772861    0.491317   negative    0.189637   negative  lexi    0.491317   \n1772862    0.546551   negative    0.311515   negative  lexi    0.546551   \n1772863    0.568371   positive    0.416277   negative  lexi    0.568371   \n\n        vote_label  \n0         positive  \n1         positive  \n2         positive  \n3         positive  \n4         negative  \n...            ...  \n1772859   negative  \n1772860   positive  \n1772861   negative  \n1772862   negative  \n1772863   positive  \n\n[1772864 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>newspaper</th>\n      <th>variable</th>\n      <th>value</th>\n      <th>lexi_score</th>\n      <th>lexi_label</th>\n      <th>bert_score</th>\n      <th>bert_label</th>\n      <th>vote</th>\n      <th>vote_score</th>\n      <th>vote_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Handel: \"Viele brauchen ja eigentlich nichts m...</td>\n      <td>zeit</td>\n      <td>Comment0</td>\n      <td>\\nSehr schönes Interview, Danke!\\nEs ist eben ...</td>\n      <td>0.606474</td>\n      <td>positive</td>\n      <td>0.197398</td>\n      <td>neutral</td>\n      <td>lexi</td>\n      <td>0.606474</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DGB-Index Gute Arbeit: An den Kosten im Homeof...</td>\n      <td>zeit</td>\n      <td>Comment0</td>\n      <td>\\nHome Office wo und wann immer möglich UND fa...</td>\n      <td>0.716222</td>\n      <td>positive</td>\n      <td>0.395624</td>\n      <td>positive</td>\n      <td>lexi</td>\n      <td>0.716222</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Coronavirus: RKI registriert erstmals wieder l...</td>\n      <td>zeit</td>\n      <td>Comment0</td>\n      <td>\\nWie ich es voraus gesagt habe.\\nEs zeichnete...</td>\n      <td>0.581695</td>\n      <td>positive</td>\n      <td>0.561593</td>\n      <td>positive</td>\n      <td>lexi</td>\n      <td>0.581695</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bund-Länder-Runde: Kanzleramtschef fordert Cor...</td>\n      <td>zeit</td>\n      <td>Comment0</td>\n      <td>\\nEntfernt. Bitte verzichten Sie auf Unterstel...</td>\n      <td>0.568505</td>\n      <td>positive</td>\n      <td>0.323396</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.568505</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Corona-Impfung für Kinder: Gesundheitsminister...</td>\n      <td>zeit</td>\n      <td>Comment0</td>\n      <td>\\nWarum schafft man eigentlich nicht endlich d...</td>\n      <td>0.539759</td>\n      <td>negative</td>\n      <td>0.397975</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.539759</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1772859</th>\n      <td>Inzidenz in Berlin steigt wieder leicht</td>\n      <td>tagespiegel</td>\n      <td>Comment6184</td>\n      <td>Nach allem, was man hinsichtlich der Entwicklu...</td>\n      <td>0.530947</td>\n      <td>negative</td>\n      <td>0.249955</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.530947</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1772860</th>\n      <td>Inzidenz in Berlin steigt wieder leicht</td>\n      <td>tagespiegel</td>\n      <td>Comment6185</td>\n      <td>Man kann nur hoffen,daß dem Berliner Senat mög...</td>\n      <td>0.568764</td>\n      <td>positive</td>\n      <td>0.236482</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.568764</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1772861</th>\n      <td>Inzidenz in Berlin steigt wieder leicht</td>\n      <td>tagespiegel</td>\n      <td>Comment6186</td>\n      <td>Massentest bringen natürlich etwas: Die Verlau...</td>\n      <td>0.491317</td>\n      <td>negative</td>\n      <td>0.189637</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.491317</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1772862</th>\n      <td>Inzidenz in Berlin steigt wieder leicht</td>\n      <td>tagespiegel</td>\n      <td>Comment6187</td>\n      <td>Wenn schon Shutdown, dann richtig. Dann ist di...</td>\n      <td>0.546551</td>\n      <td>negative</td>\n      <td>0.311515</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.546551</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1772863</th>\n      <td>Inzidenz in Berlin steigt wieder leicht</td>\n      <td>tagespiegel</td>\n      <td>Comment6188</td>\n      <td>Unser kleiner Blumenladen, in CH, muss demnach...</td>\n      <td>0.568371</td>\n      <td>positive</td>\n      <td>0.416277</td>\n      <td>negative</td>\n      <td>lexi</td>\n      <td>0.568371</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>1772864 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_comments_senti"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load command\n",
    "with mgzip.open('../data/combined_comments_bert.mgzip', 'rb') as handle:\n",
    "    combined_comments_bert = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                       bert  bert_conf  \\\n0         ([[0.34781762957572937, -1.2522820234298706, 1...   1.277788   \n1         ([[4.476775169372559, -1.91838538646698, -2.86...   2.558390   \n2         ([[3.1189382076263428, 0.8245750069618225, -4....   3.630590   \n3         ([[-1.203690528869629, 3.295466184616089, -2.6...   2.091776   \n4         ([[-1.3200560808181763, 3.8936355113983154, -3...   2.573579   \n...                                                     ...        ...   \n76941399  ([[-1.5527026653289795, 2.864715576171875, -1....   1.595653   \n76953839  ([[-1.5480496883392334, 3.0557596683502197, -1...   1.510323   \n76966279  ([[-1.0233427286148071, 2.2369704246520996, -1...   1.213628   \n76978719  ([[-1.4645229578018188, 3.450071096420288, -2....   1.985548   \n76991159  ([[-2.2502105236053467, 3.713120222091675, -1....   2.649063   \n\n         bert_label  bert_score  \n0           neutral    0.166603  \n1          positive    0.996023  \n2          positive    0.816437  \n3          negative   -0.975590  \n4          negative   -0.988513  \n...             ...         ...  \n76941399   negative   -0.960944  \n76953839   negative   -0.970524  \n76966279   negative   -0.903010  \n76978719   negative   -0.982647  \n76991159   negative   -0.986585  \n\n[1772864 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bert</th>\n      <th>bert_conf</th>\n      <th>bert_label</th>\n      <th>bert_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>([[0.34781762957572937, -1.2522820234298706, 1...</td>\n      <td>1.277788</td>\n      <td>neutral</td>\n      <td>0.166603</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>([[4.476775169372559, -1.91838538646698, -2.86...</td>\n      <td>2.558390</td>\n      <td>positive</td>\n      <td>0.996023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>([[3.1189382076263428, 0.8245750069618225, -4....</td>\n      <td>3.630590</td>\n      <td>positive</td>\n      <td>0.816437</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>([[-1.203690528869629, 3.295466184616089, -2.6...</td>\n      <td>2.091776</td>\n      <td>negative</td>\n      <td>-0.975590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>([[-1.3200560808181763, 3.8936355113983154, -3...</td>\n      <td>2.573579</td>\n      <td>negative</td>\n      <td>-0.988513</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76941399</th>\n      <td>([[-1.5527026653289795, 2.864715576171875, -1....</td>\n      <td>1.595653</td>\n      <td>negative</td>\n      <td>-0.960944</td>\n    </tr>\n    <tr>\n      <th>76953839</th>\n      <td>([[-1.5480496883392334, 3.0557596683502197, -1...</td>\n      <td>1.510323</td>\n      <td>negative</td>\n      <td>-0.970524</td>\n    </tr>\n    <tr>\n      <th>76966279</th>\n      <td>([[-1.0233427286148071, 2.2369704246520996, -1...</td>\n      <td>1.213628</td>\n      <td>negative</td>\n      <td>-0.903010</td>\n    </tr>\n    <tr>\n      <th>76978719</th>\n      <td>([[-1.4645229578018188, 3.450071096420288, -2....</td>\n      <td>1.985548</td>\n      <td>negative</td>\n      <td>-0.982647</td>\n    </tr>\n    <tr>\n      <th>76991159</th>\n      <td>([[-2.2502105236053467, 3.713120222091675, -1....</td>\n      <td>2.649063</td>\n      <td>negative</td>\n      <td>-0.986585</td>\n    </tr>\n  </tbody>\n</table>\n<p>1772864 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_comments_bert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}