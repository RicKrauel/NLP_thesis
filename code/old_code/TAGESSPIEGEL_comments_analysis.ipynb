{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "from HanTa import HanoverTagger as ht\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import top2vec\n",
    "import torch\n",
    "from itertools import chain\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from typing import List\n",
    "import re\n",
    "import pickle\n",
    "import mgzip\n",
    "\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# tagesspiegel Dataset comments sentiment classification\n",
    "## Preprocessing\n",
    "First, the original scraped text file is loaded (note: I use gzip compression due to the big size of the files). Since tThe original file is in the wide format, I use the melt function of pandas to transpose the data into the long format. This enables to compare the methods of sentiment classification more easily later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagesspiegel = pd.read_csv('../data/tagesspiegel_scraped.gzip', compression='gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# columns initially scraped but not necessary for the analysis anymore are deleted\n",
    "# the article text is removed since this notebook is only for the sentiment analysis\n",
    "# the article title serves as the ID of each article, since it is shorter than the article text\n",
    "tagesspiegel.drop(['Unnamed: 0', 'combined_text', 'link'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comment_cols = [col for col in tagesspiegel.columns if 'Comment' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel = pd.melt(tagesspiegel, id_vars=['title', 'date'], value_vars=comment_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization & punctuation removal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel['tokens'] = tagesspiegel['value'].progress_apply(nltk.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_list = []\n",
    "    for word in text:\n",
    "        for punctuation in string.punctuation:\n",
    "            word = word.replace(punctuation, '')\n",
    "        if not word:\n",
    "            continue\n",
    "        text_list.append(word.lower())\n",
    "    return text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel['tokens'] = tagesspiegel['tokens'].progress_apply(remove_punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stop word removal & lemmatization\n",
    "Following, stopwords without meaning ('der', 'wo', etc.) are removed from the comments. I edited the sourced stopwords list and removed any negations, since these contain emotions that I want to capture later in the workflow. Afterwards, the remaining tokenized words for each comment are lemmatized, i.e. reduced to their basic form. In order to do this I utilise the 'Hannover Tagger', which also provides part of speech information, although I do not use it (the main reason that I do not use the PoS information is that the utilised German emotion and sentiment lexical use different PoS abbreviations, resulting in no matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = open('../resources/german_stopwords-master/german_stopwords_plain.txt').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stop_word_removal(x):\n",
    "    return list([w for w in x if not w in stopwords])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel['tokens'] = tagesspiegel['tokens'].progress_apply(stop_word_removal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tagger_custom(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        tmp_list.append(tagger.analyze(word))\n",
    "    return tmp_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel['tokens'] = tagesspiegel['tokens'].progress_apply(tagger_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mgzip.open(\"../data/tagesspiegel_comments_preprocessed.mgzip\", 'wb') as f:\n",
    "    pickle.dump(tagesspiegel, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# open from disk\n",
    "with mgzip.open('../data/tagesspiegel_comments_preprocessed.mgzip', 'rb') as handle:\n",
    "    tagesspiegel_pre = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization of emotion lexicon\n",
    "The utilised emotion lexicon by Robert Klinger is also lemmatized, in order to be able to match the lemmatized words in the comments to the ones in the lexicals. Note: I used an IF statement per lexicon since I did not manage to make it work in a combined FOR loop."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ekel = open('../resources/german-emotion-dictionary/Ekel.txt').read().splitlines()\n",
    "freude = open('../resources/german-emotion-dictionary/Freude.txt').read().splitlines()\n",
    "furcht = open('../resources/german-emotion-dictionary/Furcht.txt').read().splitlines()\n",
    "trauer = open('../resources/german-emotion-dictionary/Trauer.txt').read().splitlines()\n",
    "surprise = open('../resources/german-emotion-dictionary/Ueberraschung.txt').read().splitlines()\n",
    "verachtung = open('../resources/german-emotion-dictionary/Verachtung.txt').read().splitlines()\n",
    "wut = open('../resources/german-emotion-dictionary/Wut.txt').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp_list = []\n",
    "for words in ekel:\n",
    "   tmp_list.append(tagger.analyze(words))\n",
    "ekel = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in ekel:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "ekel = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in freude:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "freude = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in freude:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "freude = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in furcht:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "furcht = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in furcht:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "furcht = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in trauer:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "trauer = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in trauer:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "trauer = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in surprise:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "surprise = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in surprise:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "surprise = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in verachtung:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "verachtung = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in verachtung:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "verachtung = tmp_list\n",
    "\n",
    "tmp_list = []\n",
    "for words in wut:\n",
    "    tmp_list.append(tagger.analyze(words))\n",
    "wut = tmp_list\n",
    "tmp_list = []\n",
    "for emotion in wut:\n",
    "    x, y = emotion\n",
    "    tmp_list.append(x.lower())\n",
    "wut = tmp_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lexicon emotion classification (comments)\n",
    "In this step, I assign every word in the tokenized and lemmatized comments to an emotion (if there is a match). Afterwards, for every comment the dominant emotion is detected and assigned as the overall emotion of the comment. This leaves me with one emotion per comment, which are added into one large list per news article. The resulting file is saved with the ending '_emo_clas'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def emo_class(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        if not word:\n",
    "            continue\n",
    "        x, y  = word\n",
    "        if str(x).lower() in ekel:\n",
    "            tmp_list.append('ekel')\n",
    "        if str(x).lower() in freude:\n",
    "            tmp_list.append('freude')\n",
    "        if str(x).lower() in furcht:\n",
    "            tmp_list.append('furcht')\n",
    "        if str(x).lower() in trauer:\n",
    "            tmp_list.append('trauer')\n",
    "        if str(x).lower() in surprise:\n",
    "            tmp_list.append('surprise')\n",
    "        if str(x).lower() in verachtung:\n",
    "            tmp_list.append('verachtung')\n",
    "        if str(x).lower() in wut:\n",
    "            tmp_list.append('wut')\n",
    "    if not tmp_list:\n",
    "        return None\n",
    "    return Counter(tmp_list).most_common(1)[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['emotion'] = tagesspiegel_pre['tokens'].progress_apply(emo_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment classification with lexicon (comments)\n",
    "Similar to the previous step, the comments are also assigned to negative or positive sentiment overall, in a similar fashion. The result is saved in a new file with the ending '_senti_clas'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "senti_merge = pd.read_csv('../resources/sentimerge/data/sentimerge.txt', sep='\\t')\n",
    "senti_merge.drop(['PoS'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "senti_merge_pos = senti_merge.loc[senti_merge['sentiment'] > 0]\n",
    "senti_merge_neg = senti_merge.loc[senti_merge['sentiment'] < 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "senti_pos_dict = dict(zip(senti_merge_pos['lemma'], senti_merge_pos['sentiment']))\n",
    "senti_neg_dict = dict(zip(senti_merge_neg['lemma'], senti_merge_neg['sentiment']))\n",
    "senti_weight_dict = dict(zip(senti_merge['lemma'], senti_merge['weight']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def senti_class(input):\n",
    "    tmp_list = []\n",
    "    for word in input:\n",
    "        if not word:\n",
    "            continue\n",
    "        x, y  = word\n",
    "        if str(x).lower() in senti_pos_dict:\n",
    "            tmp_list.append((senti_pos_dict[x.lower()]*senti_weight_dict[x.lower()]))\n",
    "        if str(x).lower() in senti_neg_dict:\n",
    "            tmp_list.append((senti_neg_dict[x.lower()]*senti_weight_dict[x.lower()]))\n",
    "    if not tmp_list:\n",
    "        return np.nan\n",
    "    return sum(tmp_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def rescale(input):\n",
    "    scale = input\n",
    "    scale -= scale.min()\n",
    "    scale /= scale.max()\n",
    "    return scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lexi_label(input):\n",
    "    if input > 1:\n",
    "        return 'positive'\n",
    "    elif input < -1:\n",
    "        return 'negative'\n",
    "    elif pd.isna(input):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return 'neutral'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['lexi_score'] = tagesspiegel_pre['tokens'].progress_apply(senti_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['lexi_label'] = tagesspiegel_pre['lexi_score'].apply(lexi_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['lexi_score'] = rescale(tagesspiegel_pre['lexi_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(tagesspiegel_pre, x=\"lexi_score\", nbins=1000)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_lexi = tagesspiegel_pre[['title', 'value', 'lexi_score', 'lexi_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving to disk\n",
    "with mgzip.open('../data/tagesspiegel_lexi.mgzip', 'wb') as handle:\n",
    "    pickle.dump(tagesspiegel_lexi, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment classification with BERT (comments)\n",
    "##### File was produced on Google Colab due to GPU acceleration, saved as 'tagesspiegel_senti_clas_bert.gzip'.\n",
    "In this step, the comments are classified for sentiment using the pretrained BERT model \"GermanSentiment\". This model was trained on 1.8 Million German language samples from various domains. This serves as a second method next to the lexicon approach and will be used to compare both methods. I modified the given code to also return the confidence logit values for each prediction. These are used to compare this and the lexicon methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SentimentModel():\n",
    "    def __init__(self, model_name: str):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
    "        self.clean_http_urls = re.compile(r'https*\\\\S+', re.MULTILINE)\n",
    "        self.clean_at_mentions = re.compile(r'@\\\\S+', re.MULTILINE)\n",
    "\n",
    "    def predict_sentiment(self, texts):\n",
    "        if np.any(pd.isna(texts)):\n",
    "          return np.nan\n",
    "        texts = [self.clean_text(str(text)) for text in [texts]]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        encoded = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True,truncation=True, return_tensors=\"pt\")\n",
    "        encoded = encoded.to(self.device)\n",
    "        with torch.no_grad():\n",
    "                logits = self.model(**encoded)\n",
    "\n",
    "        label_ids = torch.argmax(logits[0], axis=1)\n",
    "        for a in zip(logits[0].tolist(), [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]):\n",
    "            tmp_rsl = list(a)\n",
    "        return tmp_rsl\n",
    "\n",
    "    def replace_numbers(self,text: str) -> str:\n",
    "            return text.replace(\"0\",\" null\").replace(\"1\",\" eins\").replace(\"2\",\" zwei\").replace(\"3\",\" drei\").replace(\"4\",\" vier\").replace(\"5\",\" fünf\").replace(\"6\",\" sechs\").replace(\"7\",\" sieben\").replace(\"8\",\" acht\").replace(\"9\",\" neun\")\n",
    "\n",
    "    def clean_text(self,text: str)-> str:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text = self.clean_http_urls.sub('',text)\n",
    "            text = self.clean_at_mentions.sub('',text)\n",
    "            text = self.replace_numbers(text)\n",
    "            text = self.clean_chars.sub('', text) # use only text chars\n",
    "            text = ' '.join(text.split()) # substitute multiple whitespace with single whitespace\n",
    "            text = text.strip().lower()\n",
    "            return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SentimentModel(model_name = \"oliverguhr/german-sentiment-bert\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['bert'] = tagesspiegel_pre['value'].progress_apply(model.predict_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bert_score(input):\n",
    "    return np.ptp(sorted(list(map(abs, input[0]))))\n",
    "\n",
    "def bert_label(input):\n",
    "    return input[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['bert_score'] = tagesspiegel_pre['bert'].apply(bert_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_pre['bert_label'] = tagesspiegel_pre['bert'].apply(bert_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rescale(tagesspiegel_pre['bert_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tagesspiegel_bert = tagesspiegel_pre[['title', 'bert_score', 'bert_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mgzip.open(\".../data/tagesspiegel_bert.mgzip\", 'wb') as f:\n",
    "    pickle.dump(tagesspiegel_bert, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSentiA comments (comparison of lexicon & BERT methods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting labels\n",
    "For this step, I compare the predictions of both prior methods and pick the ones with the highest confident value. Further, the high confident predictions will be used to train other ML models to improve low confidence predictions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load command\n",
    "with mgzip.open('../data/tagesspiegel_lexi.mgzip', 'rb') as handle:\n",
    "    tagesspiegel_lexi = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load command\n",
    "with mgzip.open('../data/tagesspiegel_bert.mgzip', 'rb') as handle:\n",
    "    tagesspiegel_bert = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tagesspiegel_senti_compare = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tagesspiegel_senti_compare[['title', 'comment', 'lexi_score','lexi_label']] = tagesspiegel_lexi[['title', 'value', 'lexi_score','lexi_label']]\n",
    "tagesspiegel_senti_compare[['bert_score','bert_label']] = tagesspiegel_bert[['bert_score','bert_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tagesspiegel_senti_compare['vote'] = np.where((tagesspiegel_senti_compare['lexi_score'] > tagesspiegel_senti_compare['bert_score']), 'lexi', 'bert')\n",
    "tagesspiegel_senti_compare['vote_score'] = np.where((tagesspiegel_senti_compare['lexi_score'] > tagesspiegel_senti_compare['bert_score']), tagesspiegel_senti_compare['lexi_score'], tagesspiegel_senti_compare['bert_score'])\n",
    "tagesspiegel_senti_compare['vote_label'] = np.where((tagesspiegel_senti_compare['lexi_score'] > tagesspiegel_senti_compare['bert_score']), tagesspiegel_senti_compare['lexi_label'], tagesspiegel_senti_compare['bert_label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tagesspiegel_senti_compare.drop(['lexi_score','lexi_label', 'bert_label', 'bert_score'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving to disk\n",
    "with mgzip.open('../data/tagesspiegel_ssentia.mgzip', 'wb') as handle:\n",
    "    pickle.dump(tagesspiegel_senti_compare, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load command\n",
    "with mgzip.open('../data/tagesspiegel_ssentia.mgzip', 'rb') as handle:\n",
    "    tagesspiegel_senti_compare = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train = tagesspiegel_senti_compare[tagesspiegel_senti_compare['vote_score'] > tagesspiegel_senti_compare['vote_score'].mean() + 0.05 * tagesspiegel_senti_compare['vote_score'].std()]\n",
    "test = tagesspiegel_senti_compare[tagesspiegel_senti_compare['vote_score'] < tagesspiegel_senti_compare['vote_score'].mean() + 0.05 * tagesspiegel_senti_compare['vote_score'].std()]\n",
    "train.drop(['title', 'vote_score', 'vote'], axis=1, inplace=True)\n",
    "test.drop(['title', 'vote_score', 'vote'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X_train = train.loc[:, train.columns != 'vote_label']\n",
    "y_train = train['vote_label']\n",
    "X_test = test.loc[:, test.columns != 'vote_label']\n",
    "y_test = test['vote_label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(0,1), tokenizer=lambda x: x.split())\n",
    "X_train = vectorizer.fit_transform(X_train['comment'])\n",
    "X_test = vectorizer.transform(X_test['comment'])\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [49]\u001B[0m, in \u001B[0;36m<cell line: 31>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m rf_random \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(estimator \u001B[38;5;241m=\u001B[39m rf, param_distributions \u001B[38;5;241m=\u001B[39m random_grid, n_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m , cv \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, verbose \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_jobs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Fit the random search model\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m \u001B[43mrf_random\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m rf_random\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:891\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    885\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    886\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    887\u001B[0m     )\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 891\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    895\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1766\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1765\u001B[0m     \u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1766\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1768\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m   1769\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:838\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    832\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    833\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    834\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    835\u001B[0m         )\n\u001B[1;32m    836\u001B[0m     )\n\u001B[0;32m--> 838\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    857\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    860\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/parallel.py:1056\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1056\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/parallel.py:935\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    934\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 935\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    937\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/_parallel_backends.py:542\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 542\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/concurrent/futures/_base.py:439\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m--> 439\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/threading.py:302\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 302\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "# Criterion\n",
    "criterion =['gini', 'entropy']\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt', 'auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': criterion,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5 , cv = 3, verbose = 3, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)\n",
    "\n",
    "rf_random.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 670 out of 670 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=8)]: Done 670 out of 670 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.1771346752209709"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regr = RandomForestClassifier(verbose=1, n_jobs=-1)\n",
    "regr = RandomForestClassifier(verbose = 1, n_jobs = -1, n_estimators = 670, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 89, criterion = 'entropy')\n",
    "regr.fit(X_train,y_train)\n",
    "regr.score(X_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# predict labels for the whole data\n",
    "X = tagesspiegel_senti_compare['comment']\n",
    "X = vectorizer.transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=8)]: Done 670 out of 670 | elapsed:    9.8s finished\n"
     ]
    }
   ],
   "source": [
    "tagesspiegel_senti_compare['ssentia_label'] = regr.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tagesspiegel_senti_compare['ssentia_label'] = np.where(tagesspiegel_senti_compare['ssentia_label'] == 2, 'positive', np.where(tagesspiegel_senti_compare['ssentia_label'] == 1, 'neutral', 'negative'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding most numerous mode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tagesspiegel_group = tagesspiegel_senti_compare[['title', 'vote_label', 'vote_score', 'ssentia_label']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_40420/3627271235.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tagesspiegel_group['vote_value'] = np.where(tagesspiegel_group['vote_label'] == 'positive', 3, np.where(tagesspiegel_group['vote_label'] == 'neutral', 2, 1))\n"
     ]
    }
   ],
   "source": [
    "tagesspiegel_group['vote_value'] = np.where(tagesspiegel_group['vote_label'] == 'positive', 3, np.where(tagesspiegel_group['vote_label'] == 'neutral', 2, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# general grouping\n",
    "#tagesspiegel_group = tagesspiegel_group.groupby('title', as_index = False)[['vote_label', 'ssentia_label']].agg(vote=('vote_label', lambda x: pd.Series.mode(x)[0]), ssentia=('ssentia_label', lambda x: pd.Series.mode(x)[0]), count=('vote_label', np.count_nonzero))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_40420/1941239966.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  tagesspiegel_group = tagesspiegel_group.groupby(['title', 'vote_label'], as_index = False)['vote_value', 'vote_label', 'vote_score'].agg(value = ('vote_value', np.mean),count=('vote_label', np.count_nonzero),conf_value=('vote_score', np.mean))\n"
     ]
    }
   ],
   "source": [
    "# vote_value grouping with mean\n",
    "tagesspiegel_group = tagesspiegel_group.groupby(['title', 'vote_label'], as_index = False)['vote_value', 'vote_label', 'vote_score'].agg(value = ('vote_value', np.mean),count=('vote_label', np.count_nonzero),conf_value=('vote_score', np.mean))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   title vote_label  value  \\\n0       Deutsche Bank verweigert Auskunft zu Trump-Ge...   negative    1.0   \n1       Deutsche Bank verweigert Auskunft zu Trump-Ge...    neutral    2.0   \n2                     \"500 Euro reichen nicht zum Leben\"   negative    1.0   \n3                     \"500 Euro reichen nicht zum Leben\"   positive    3.0   \n4         \"Alarmstufe Rot\" für die Veranstaltungsbranche   negative    1.0   \n...                                                  ...        ...    ...   \n21534  „Über unsere Arbeitsbedingungen will niemand m...    neutral    2.0   \n21535  „Über unsere Arbeitsbedingungen will niemand m...   positive    3.0   \n21536  „Überlegen, ob er stellvertretender Ministerpr...   negative    1.0   \n21537  „Überlegen, ob er stellvertretender Ministerpr...    neutral    2.0   \n21538  „Überlegen, ob er stellvertretender Ministerpr...   positive    3.0   \n\n       count  conf_value  log_count  anteil_intern     final      test  \n0          1    0.541383   0.000000       0.500000  0.000000  0.000000  \n1          1    0.571705   0.000000       0.500000  0.000000  0.000000  \n2          1    0.502691   0.000000       0.500000  0.000000  0.000000  \n3          1    0.580092   0.000000       0.500000  0.000000  0.000000  \n4          1    0.553421   0.000000       0.500000  0.000000  0.000000  \n...      ...         ...        ...            ...       ...       ...  \n21534      3    0.591422   1.098612       0.300000  0.603057  0.574094  \n21535      3    0.634644   1.098612       0.300000  0.783105  0.574094  \n21536      7    0.537796   1.945910       0.636364  0.690239  1.112792  \n21537      2    0.653186   0.693147       0.181818  0.302386  0.355002  \n21538      2    0.643270   0.693147       0.181818  0.469871  0.355002  \n\n[21539 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>vote_label</th>\n      <th>value</th>\n      <th>count</th>\n      <th>conf_value</th>\n      <th>log_count</th>\n      <th>anteil_intern</th>\n      <th>final</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Deutsche Bank verweigert Auskunft zu Trump-Ge...</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.541383</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Deutsche Bank verweigert Auskunft zu Trump-Ge...</td>\n      <td>neutral</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0.571705</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"500 Euro reichen nicht zum Leben\"</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.502691</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"500 Euro reichen nicht zum Leben\"</td>\n      <td>positive</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0.580092</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"Alarmstufe Rot\" für die Veranstaltungsbranche</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.553421</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21534</th>\n      <td>„Über unsere Arbeitsbedingungen will niemand m...</td>\n      <td>neutral</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>0.591422</td>\n      <td>1.098612</td>\n      <td>0.300000</td>\n      <td>0.603057</td>\n      <td>0.574094</td>\n    </tr>\n    <tr>\n      <th>21535</th>\n      <td>„Über unsere Arbeitsbedingungen will niemand m...</td>\n      <td>positive</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>0.634644</td>\n      <td>1.098612</td>\n      <td>0.300000</td>\n      <td>0.783105</td>\n      <td>0.574094</td>\n    </tr>\n    <tr>\n      <th>21536</th>\n      <td>„Überlegen, ob er stellvertretender Ministerpr...</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>0.537796</td>\n      <td>1.945910</td>\n      <td>0.636364</td>\n      <td>0.690239</td>\n      <td>1.112792</td>\n    </tr>\n    <tr>\n      <th>21537</th>\n      <td>„Überlegen, ob er stellvertretender Ministerpr...</td>\n      <td>neutral</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>0.653186</td>\n      <td>0.693147</td>\n      <td>0.181818</td>\n      <td>0.302386</td>\n      <td>0.355002</td>\n    </tr>\n    <tr>\n      <th>21538</th>\n      <td>„Überlegen, ob er stellvertretender Ministerpr...</td>\n      <td>positive</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>0.643270</td>\n      <td>0.693147</td>\n      <td>0.181818</td>\n      <td>0.469871</td>\n      <td>0.355002</td>\n    </tr>\n  </tbody>\n</table>\n<p>21539 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagesspiegel_group"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# log of counts in order to model the amount of comments it got but not overemphasize outliers too much\n",
    "tagesspiegel_group['log_count'] = np.log(tagesspiegel_group['count'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "tagesspiegel_group['anteil_intern'] = tagesspiegel_group['count'] / tagesspiegel_group.groupby('title')['count'].transform('sum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "tagesspiegel_group['test'] = np.sqrt(tagesspiegel_group['log_count']*tagesspiegel_group['anteil_intern'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "tagesspiegel_group['final'] = (np.log(tagesspiegel_group['conf_value'])*(-1))*tagesspiegel_group['value']*np.sqrt(tagesspiegel_group['log_count']*tagesspiegel_group['anteil_intern'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/ng7ccz8d4fdfcgp2wp0g434w0000gn/T/ipykernel_40420/793224451.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  tagesspiegel_group_value = tagesspiegel_group.groupby(['title'], as_index = False)['value', 'count', 'final'].agg(calc=('final', np.mean), value=('value', np.mean), count=('count', np.sum))\n"
     ]
    }
   ],
   "source": [
    "tagesspiegel_group_value = tagesspiegel_group.groupby(['title'], as_index = False)['value', 'count', 'final'].agg(calc=('final', np.mean), value=('value', np.mean), count=('count', np.sum))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  title      calc  value  \\\n0      Deutsche Bank verweigert Auskunft zu Trump-Ge...  0.000000    1.5   \n1                    \"500 Euro reichen nicht zum Leben\"  0.000000    2.0   \n2        \"Alarmstufe Rot\" für die Veranstaltungsbranche  0.000000    1.5   \n3            \"An Willkommenskultur war nicht zu denken\"  0.580165    2.0   \n4     \"Breite Teile der Bevölkerung werden in Not ge...  0.657748    2.0   \n...                                                 ...       ...    ...   \n9424  „Ältere und Kranke werden ihre Kontakte deutli...  0.996207    2.0   \n9425           „Ärzte können keinen Völkermord stoppen“  0.000000    2.0   \n9426  „Ökozid“ – Frankreich schafft neuen Straftatbe...  0.434294    2.0   \n9427  „Über unsere Arbeitsbedingungen will niemand m...  0.616820    2.0   \n9428  „Überlegen, ob er stellvertretender Ministerpr...  0.487499    2.0   \n\n      count  \n0         2  \n1         2  \n2         2  \n3        14  \n4         9  \n...     ...  \n9424     62  \n9425      1  \n9426      8  \n9427     10  \n9428     11  \n\n[9429 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>calc</th>\n      <th>value</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Deutsche Bank verweigert Auskunft zu Trump-Ge...</td>\n      <td>0.000000</td>\n      <td>1.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"500 Euro reichen nicht zum Leben\"</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"Alarmstufe Rot\" für die Veranstaltungsbranche</td>\n      <td>0.000000</td>\n      <td>1.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"An Willkommenskultur war nicht zu denken\"</td>\n      <td>0.580165</td>\n      <td>2.0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"Breite Teile der Bevölkerung werden in Not ge...</td>\n      <td>0.657748</td>\n      <td>2.0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9424</th>\n      <td>„Ältere und Kranke werden ihre Kontakte deutli...</td>\n      <td>0.996207</td>\n      <td>2.0</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>9425</th>\n      <td>„Ärzte können keinen Völkermord stoppen“</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9426</th>\n      <td>„Ökozid“ – Frankreich schafft neuen Straftatbe...</td>\n      <td>0.434294</td>\n      <td>2.0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9427</th>\n      <td>„Über unsere Arbeitsbedingungen will niemand m...</td>\n      <td>0.616820</td>\n      <td>2.0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>9428</th>\n      <td>„Überlegen, ob er stellvertretender Ministerpr...</td>\n      <td>0.487499</td>\n      <td>2.0</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n<p>9429 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagesspiegel_group_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# saving to disk\n",
    "with mgzip.open('../data/tagesspiegel_predict_new.mgzip', 'wb') as handle:\n",
    "    pickle.dump(tagesspiegel_group_value, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svc.score(X_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [377]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.1\u001B[39m,\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m100\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0.1\u001B[39m,\u001B[38;5;241m0.01\u001B[39m,\u001B[38;5;241m0.001\u001B[39m],\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkernel\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m]}\n\u001B[1;32m      3\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(SVC(),param_grid,refit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mgrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:891\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    885\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    886\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    887\u001B[0m     )\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 891\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    895\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1392\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1391\u001B[0m     \u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1392\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/sklearn/model_selection/_search.py:838\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    832\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    833\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    834\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    835\u001B[0m         )\n\u001B[1;32m    836\u001B[0m     )\n\u001B[0;32m--> 838\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    857\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    860\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/parallel.py:1056\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1056\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/parallel.py:935\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    934\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 935\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    937\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/site-packages/joblib/_parallel_backends.py:542\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 542\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/concurrent/futures/_base.py:439\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m--> 439\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/NLP_thesis/lib/python3.8/threading.py:302\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 302\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}